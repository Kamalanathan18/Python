# -*- coding: utf-8 -*-
"""Pythonproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ALuVQLI5aYjVAbQ2z4radH-u5MtRNqpY
"""

from google.colab import files
uploaded = files.upload()

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report


# import datasets
df_seq = pd.read_csv('drive/MyDrive/pdb_data_seq.csv')
df_char = pd.read_csv('drive/MyDrive/pdb_data_no_dups.csv')
print(df_seq.head())
print(df_char.head())

print(len(df_seq))

# filter for only proteins
protein_char = df_char[df_char.macromoleculeType == 'Protein']
protein_seq = df_seq[df_seq.macromoleculeType == 'Protein']
print(protein_char['macromoleculeType'])

protein_char = protein_char[['structureId','classification']]
protein_seq = protein_seq[['structureId','sequence']]
print(protein_seq.head())
print(protein_char.head())

# join two datasets based on structure id
model_f = protein_char.set_index('structureId').join(protein_seq.set_index('structureId'))
print(model_f.head())

def missing_values(model_f):
    total = model_f.isnull().sum()
    percent = (model_f.isnull().sum()/model_f.isnull().count()*100)
    print(model_f.shape[0],'is the number of rows in the joined dataset')
    print(total)
    print(percent)
missing_values(model_f)

# Drop rows with missing values

model_f = model_f.dropna()
print(model_f.shape[0],'is the number of rows after dropped the missing values')
print(model_f.isnull().sum())

print(model_f[model_f.classification == 'HYDROLASE'])

counts = model_f.classification.value_counts()
print(counts)

def count(x):
  plt.figure()
  sns.kdeplot(data=x, color = 'red')
  plt.title('Count Distribution for Family Types')
  plt.ylabel('% of records')
  plt.ylim(0.000000, 0.000006)
  plt.show()

count(counts)

# Get classification types where counts are over 1000
types = np.asarray(counts[(counts > 1000)].index)
print(types)

print(types[42])
print(types[5])

# Filter dataset's records for classification types > 1000
data = model_f[model_f.classification.isin(types)]
print( data.shape[0],'is the number of records in the final filtered dataset')

# ----- Train Test Split ----- for ngram_range 4,4

# Split Data
X_train, X_test, y_train, y_test = train_test_split(data['sequence'], data['classification'], test_size = 0.2, random_state = 1)

# Create a Count Vectorizer to gather the unique elements in sequence
vect = CountVectorizer(analyzer = 'char_wb', ngram_range = (4,4))

# Fit and Transform CountVectorizer
vect.fit(X_train)
X_train_df = vect.transform(X_train)
X_test_df = vect.transform(X_test)

# Print a few of the features
print(vect.get_feature_names()[-10:])

# Make a prediction dictionary to store accuracys
prediction = dict()

# ------ Machine Learning Models ------

# Naive Bayes Model
from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB()
model.fit(X_train_df, y_train)
NB_pred = model.predict(X_test_df)
prediction["Naive_Bayes"] = accuracy_score(NB_pred, y_test)
print(prediction['Naive_Bayes'])

# Adaboost Model
from sklearn.ensemble import AdaBoostClassifier
model = AdaBoostClassifier()
model.fit(X_train_df,y_train)
ADA_pred = model.predict(X_test_df)
prediction["Adaboost"] = accuracy_score(ADA_pred , y_test)
print(prediction["Adaboost"])

# confusion matrix for Naive Bayes
conf_mat = confusion_matrix(y_test, NB_pred, labels = types)

# Normalize confusion matrix
conf_mat = conf_mat.astype('float')/ conf_mat.sum(axis=1)[:, np.newaxis]

# Plot Heat Map
def heat_map_Naivebayes(a):
  fig , ax = plt.subplots()
  fig.set_size_inches(13, 8)
  a=plt.imshow(a, cmap='hot', interpolation='nearest')
  plt.show()

heat_map_Naivebayes(conf_mat)

# confusion matrix for Adaboost
conf_mat = confusion_matrix(y_test, ADA_pred, labels = types)

# Normalize confusion matrix
conf_mat = conf_mat.astype('float')/ conf_mat.sum(axis=1)[:, np.newaxis]

# Plot Heat Map
def heat_map_Adaboost(a):
  fig , ax = plt.subplots()
  fig.set_size_inches(13, 8)
  a=plt.imshow(a, cmap='hot', interpolation='nearest')
  plt.show()

heat_map_Adaboost(conf_mat)

# ----- Train Test Split ----- for ngram_range 5,5

# Split Data
X_train2, X_test2,y_train2,y_test2 = train_test_split(data['sequence'], data['classification'], test_size = 0.2, random_state = 1)

# Create a Count Vectorizer to gather the unique elements in sequence 
vect2 = CountVectorizer(analyzer = 'char_wb', ngram_range = (5,5))

# Fit and Transform CountVectorizer
vect2.fit(X_train2)
X_train_df2 = vect2.transform(X_train2)
X_test_df2 = vect2.transform(X_test2)

# Print a few of the features
print(vect2.get_feature_names()[-20:])

# Naive Bayes Model
from sklearn.naive_bayes import MultinomialNB
model2 = MultinomialNB()
model2.fit(X_train_df2, y_train2)
NB_pred2 = model2.predict(X_test_df2)
prediction["Naivebayes"] = accuracy_score(NB_pred2, y_test2)
print( prediction['Naivebayes'])

# confusion matrix for Naive Bayes
conf_mat2 = confusion_matrix(y_test2, NB_pred2, labels = types)

# Normalize confusion_matrix
conf_mat2 = conf_mat2.astype('float')/ conf_mat2.sum(axis=1)[:, np.newaxis]

# Plot Heat Map
def heat_map_NB(a):
  fig , ax = plt.subplots()
  fig.set_size_inches(13, 8)
  a=plt.imshow(a, cmap='hot', interpolation='nearest')
  plt.show()

heat_map_NB(conf_mat2)

# Adaboost
from sklearn.ensemble import AdaBoostClassifier
model2 = AdaBoostClassifier()
model2.fit(X_train_df2,y_train2)
ADA_pred2 = model2.predict(X_test_df2)
prediction["Ada_boost"] = accuracy_score(ADA_pred2 , y_test2)
print(prediction["Ada_boost"])

# confusion matrix for Adaboost
conf_mat2 = confusion_matrix(y_test2, ADA_pred2, labels = types)

# Normalize confusion_matrix
conf_mat2 = conf_mat2.astype('float')/ conf_mat2.sum(axis=1)[:, np.newaxis]

# Plot Heat Map
def heat_map_ADA(a):
  fig , ax = plt.subplots()
  fig.set_size_inches(13, 8)
  a=plt.imshow(a, cmap='hot', interpolation='nearest')
  plt.show()

heat_map_ADA(conf_mat2)

# classification report for Naive bayes 4,4 ngram_range
print(classification_report(y_test, NB_pred, target_names = types))

# classification report for Naive bayes 5,5 ngram_range
print(classification_report(y_test2, NB_pred2, target_names = types))

print(prediction)

def distribution(x):
  keys = x.keys()
  values = x.values()
  plt.bar(keys, values)
  plt.title("Accuracy scores for Machine Learning Models")
  plt.xlabel("ML algorithms")
  plt.ylabel("accuracy-score")
  plt.show()

distribution(prediction)